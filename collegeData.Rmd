---
title: "College Data"
Authour: Amara Diallo
output:
  word_document: default
  html_notebook: default
  pdf_document: default
---

### Introduction ----

This data contain earnings and information of college graduate based on their fields of study, major. In this project, I will try to do some exploratory analysis.I will try do a prediction on the salary if the time allow me

### let's start by loading this below packages and our data ----

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggplot2)

college_grad <- read.csv("C:/Users/Amara Diallo/Desktop/college_data.txt")
```

### Let's convert our column in capital letter ---

```{r}
college_grad <- college_grad %>%
  set_names(names(.) %>% 
              str_to_title()) 


glimpse(college_grad)
#view(college_grad)
```

### Here we will look for the Major categories that make more money upon graduation ----

In this section I will do a visualizations of the data in order to find out what major category is leading in term of salary in the job market.

```{r}

college_grad %>% 
    mutate(Major_category = fct_reorder(Major_category, Median)) %>%
    ggplot(aes(Major_category, Median)) + 
    geom_boxplot() +
    scale_y_continuous(labels = scales::dollar_format()) +
    coord_flip()
```

```{r}
college_grad %>% 
    group_by(Major_category) %>%
    summarize(Median = median(Median)) %>%
    mutate(Major_category = fct_reorder(Major_category, Median)) %>%
    ggplot(aes(Major_category, Median, fill =Median )) + 
    geom_bar(stat="identity") +
    scale_y_continuous(labels = scales::dollar_format()) +
    coord_flip()

```

As we can see above, Engineering student are the one who get more money after graduation and follow by computer&Math students. With Arts & Journalism as the lowest paying job. This finding is based on the current data we have, I am also assuming that this is probably for junior position. But, on the first graph we could point out an outlier, which mean that there is a field in the **Engineering Major** that makes a lot of money than the other field in the ***Engineering***

### In this section, we will find the highest top earning majors in all major category.----

This will help us extrapolate what is the highest major amont all major category, but it will also allow us to find out what the *OUTLIER IN ENGINEERING* we found in our early graph: The field that makes more money than all other **ENGINEERING** fields.

```{r}
Majors <- college_grad %>%
    arrange(desc(Median)) %>%
    select(Major, Major_category, Median, P25th, P75th, Sample_size)  %>%
    mutate(Major = fct_reorder(Major, Median)) 

Majors %>% head(30) %>%
    ggplot(aes(Major, Median, color = Major_category)) +
    geom_point() +
    scale_y_continuous(labels = scales::dollar_format()) +
    coord_flip()

```

Most the highest earning majors are from **ENGINEERING** field. We also realized that *PETROLEUM ENGINEERING* is not only the highest paying position in **ENGINEERING** field, but it is also the highest paying job in all major category...according to this dataset.

**ASTRONOMY & ASTROPHYSICS** is the second highest paying job, coming from the *COMPUTER & MATH* Major category; **ACTUARIAL SCIENCE** is the third and the first in the Business department;

### The lowest earning Majors ----

```{r}
college_grad %>%
    select(Major, Major_category, Median, P25th, P75th)  %>%
    tail(15) %>%
    
    mutate(Major = fct_reorder(Major, Median)) %>%
    ggplot(aes(Major, Median,color = Major_category)) +
    geom_point() +
    coord_flip()
```

The above graph shows the top 15 lowest paying job \*\* according to this dataset**. *Library Science* is the lowest paying job. As we notice, no field in the** ENGINEERING\*\* is present in this list.

```{r}
# Majors  %>%
#     ggplot(aes(Sample_size, Median)) +
#     geom_point() +
#     scale_x_log10()
# install.packages("tm")           # for text mining 
# install.packages("SnowballC")    # for text stemming 
# install.packages("wordcloud")    # word-cloud generator 
# install.packages("RColorBrewer") # color palettes 
   
# Load the packages 
# library("tm") 
# library("SnowballC") 
# library("wordcloud") 
# library("RColorBrewer")
# wordcloud(words = Majors$Major_category,  
#           freq = Majors$Median, 
#           min.freq = 1,  
#           max.words =200, 
#           random.order = TRUE,  
#           rot.per = 0.35,  
#           colors = brewer.pal(8, "Dark2")) 
```

### Most common majors ----

This part will tell us what is the major that attact most of students. We are not surprise to see that **BUSINESS** is by far the common major for college students. It is twice attractive than the rest of the major... Specially Engineering.

```{r}
college_grad %>%
    count(Major_category, wt = Total, sort = TRUE) %>%
    mutate(Major_category = fct_reorder(Major_category,n)) %>%
    ggplot(aes(Major_category, n, fill = Major_category)) +
    theme(legend.position = "none") +
    geom_col() +
    coord_flip() +
    scale_y_continuous(labels = scales::number_format()) +
    labs(y = "Number of graduates in each Major")
```

### Let's see if we can find the number of graduate in each major category, but based on **gender**. ----

```{r}
college_grad %>% 
    mutate(Major_category = fct_reorder(Major_category, Total)) %>%
    gather(Gender,Total, Men, Women) %>%
    group_by(Major_category, Gender) %>%
    #summarize(Median = median(Median)) %>%
  
    ggplot(aes(Major_category,Total, fill = Gender)) +
    geom_col() +
    facet_grid(~Gender) +
    coord_flip() +
    scale_y_continuous(labels = scales::number_format()) +
    labs(y = "Total  number of graduates by gender") 

```

It looks like *MEN* are more STERM oriented than *WOMEN*. As we can, most of the graduate students in Engineering, Computer & MATH are *MEN*. However, it is undeniable that women are ahead in health and social science related majors. Business Major is dominated by both gender.

### which gender earn more money based on top 15 Major ----

```{r}
    college_grad %>% 
    mutate(Major_category = fct_reorder(Major_category, Median)) %>%
      top_n(20, Median) %>%

    gather(Gender,Median, Men, Women) %>%
    group_by(Major_category, Gender) %>%

      ggplot(aes(Major, Median, fill = Gender)) +
    geom_col() +
    scale_y_continuous(labels = scales::dollar_format()) +
    labs(y = " ") + 

    coord_flip()
```

This above graph tells us that Men are pay more than women even though they are in the same major. this is just a dataset about recent graduate, so we can not totally rely on it. In order to really know the ins and out of this above question, we will need more dataset in order to get a better insight.

### Share of Women in each Major ----

```{r}
college_grad %>%
    group_by(Major_category, Median) %>%
    summarize_at(vars(Total, Men, Women), sum, na.rm=TRUE) %>%
    mutate(ShareWomen = Women/Total) %>%
    arrange(desc(ShareWomen)) %>%
    
    ggplot(aes(ShareWomen, Median, color = Major_category)) +
    geom_jitter() +
    scale_y_continuous(labels = scales::dollar_format()) 
```

The previous plot shows that less .25% of women make more than 50k,that is due to the fact that most of those women that are in the .25% are in the high paying major like STEM. More than .75% are below 45K, that can be explain in their major choice...like **SOCIAL SCIENCE, PSYCHOLOGY, EDUCATION, JOURNALISM ect**

#### Total Major in each major's category ----

```{r}
college_grad %>%
    select(Major, Major_category, Total, Sharewomen, Sample_size, Median) %>%
    add_count(Major_category) %>%
    filter(n >= 10) %>%
    count(Major_category) %>%
    arrange(desc(n))
```

### Let's see how much money Women get in health care profession ----

```{r}
college_grad %>% filter(Major_category == 'Health' & Sharewomen >0.7) %>%
  select(Major, Median)
#View(fresh_grad_health)
```

### Majors with the lowest unemployment rate that might be interesting for students ----

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
college_grad %>% 
  select(Major, Unemployment_rate) %>%
  filter(Unemployment_rate<0.02)
#View(fresh_grads_science)
```

### Recent graduates with median salary \> 40,000 USD where Women are represented by More than 50 percent. ----

```{r}
college_grad %>% filter(Median >=40000 & Sharewomen >.5) %>%
  select(Major, Median)

```

### CONCLUSION ----

The above analysis hels us to understand how our major's choice can have an impact on our financial status. this analysis also touch upon the choice of majors based on gender. For instance, we saw that *Men* are more attracted to STEM option, while *WOMEN* dominate health science profession.

### Predict the ShareWomen in each major ----

Let's start creating our data partitioning. 80% of our data will be used for the training set

```{r}
# Preprocessing & Sampling
library(recipes)
library(rsample)

# Standard
library(readxl)
library(tidyverse)
library(tidyquant)

# Modeling
library(parsnip)

# Plotting Decision Trees
library(rpart.plot)

```

```{r}

college_grad <- na.omit(college_grad)
set.seed(1113) # make the function reproduceable

dat_split <- rsample::initial_split(college_grad, prop = 0.80, strata = NULL)

#dat_split %>% training()
#dat_split%>% testing()

train_data <- training(dat_split)
test_data <- testing(dat_split)

```

### LINEAR METHODS ----

LINEAR REGRESSION - NO ENGINEERED FEATURES ----

.1.1 Model ----

```{r}

?linear_reg
test_data <- train_data %>% 
  bind_rows(train_data %>% filter(Major_category %>% str_detect("Interdisciplinary")))

model_01_lm <- linear_reg(mode = "regression") %>%
  set_engine("lm") %>%
  fit(Sharewomen ~  Major_category, data = train_data)

model_01_lm %>%
  predict(new_data = test_data) %>%
  
  bind_cols(Sharewomen=test_data$Sharewomen) %>%
  mutate(residuals = Sharewomen - .pred)  %>%
  yardstick::metrics(truth = Sharewomen, estimate = .pred)


```



```{r}
#model_01_lm$fit

model_01_lm$fit %>%
    broom::tidy() %>%
    arrange(p.value) %>%
    mutate(term = as_factor(term) %>% fct_rev()) %>%
    
    ggplot(aes(x = estimate, term)) + 
    geom_point() + 
    ggrepel::geom_label_repel(aes(label = scales::percent(estimate, accuracy = 1)), 
                              size = 3) + 
    scale_x_continuous(labels = scales::percent_format()) + 
    labs(
        title = "Linear Regression: Feature Importance",
        subtitle = "Model 1: Simple Linear Model"
    )

```
The intercept shows that without any features added, WomenShare is 36% in all major-category. When we start adding "Education & Health" our model changes from 36% to predicting up to 43%. However, when we added "Engineering & Math", the model abstracted WomenShare Percentages

The plot also shows that, each predictor has a coefficient that is in terms of the final output. Major_categoryArt, Major_categoryHealth, Major_categoryEducation, ect, the linear equation becomes:

y_pred = Intercept + c1 x Major_categoryArt + c2 x Major_categoryHealth + c3 x Major_categoryEducation + c4 x etc

Everything else in the model that do not have coeficent is zero because the features are not present.


```{r}
calc_metrics <- function(model, new_data = test_data){
    
    model %>%
        predict(new_data = new_data) %>%
        
        bind_cols(new_data %>% select(Sharewomen)) %>%
        mutate(residuals = Sharewomen - .pred) %>%
        
        yardstick::metrics(truth = Sharewomen, estimate = .pred)
    
}

model_01_lm %>%
    calc_metrics(test_data)
```

### LINEAR REGRESSION - WITH ENGINEERED FEATURES ----

```{r}

Model_2_lm <- linear_reg("regression") %>%
  set_engine("lm") %>%
  fit(Sharewomen~., data = train_data %>% select(-Rank, -Major,-Major_code, -Men, -Part_time ))

Model_2_lm %>%
  calc_metrics(new_data = test_data)


```


This second Linear Model has a Lower RMSE & Lower MAE (Mean Absolute Error), which  indicates better fit.
RSQ (R square): Our model explain 78% of variation within our data- The larger the R2, the better the regression model fits your observations.


```{r}
Model_2_lm$fit %>%
  broom::tidy() %>%
  arrange(p.value) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, term)) +
  geom_point() +
  ggrepel::geom_label_repel(aes(label = scales::percent(estimate, accuracy = 1)),
                            size = 3) +
  scale_x_continuous(labels = scales::percent_format())+
  labs(
    title = "Linear Regression",
    subtitle = "Linear Model 2"
  )

```

### TREE-BASED METHODS ----

### DECISION TREES ----


```{r}

#?linear_reg
#?

model_03_D_Tree <- decision_tree( mode = "regression", 
                                  cost_complexity = 0.001,
                                  tree_depth = 8,
                                  min_n = 10) %>%
  set_engine("rpart") %>%
  fit(Sharewomen~., data = train_data %>% select(-Rank, -Major,-Major_code, -Men, -Part_time ))

model_03_D_Tree %>%
  calc_metrics(new_data = test_data)

```

this above decision tree has a RSQ that is equal to 92% : Our model explain 92% of variation within our data- In addition, both of our RMSE & MAE are low, which might be a good sign so far

 **These plots are not easy to explain at all**

```{r}
model_03_D_Tree$fit %>% 
  rpart.plot(roundint = FALSE, cex = 0.6)
    
model_03_D_Tree$fit %>% 
    rpart.plot(roundint = FALSE,
               type = 2, 
               extra = 101,
               fallen.leaves = FALSE,
               cex = 0.6,
               main = 'Model 04: Decision Tree')

```

### RANDOM FOREST ----
### Model: ranger ----

```{r}
library(ranger)
#?rand_forest()
#?ranger::ranger

set.seed(1234)

model_04_rf_ranger <-rand_forest(mode = "regression", trees = 6000, min_n = 4) %>%
  set_engine("ranger", importance = "impurity") %>%
  
  fit(Sharewomen~., data = train_data %>% select(-Rank, -Major,-Major_code, -Men, -Part_time ))

model_04_rf_ranger %>%
  calc_metrics(new_data = test_data)

```

As we can see, our RSQ in the above model is really good too even though the MAE is a lit bit higher than the previous model by 0.002; If if have to choose between this model and the Tree based model, I would probably go for the Tree based model.

### ranger: Feature Importance ----

```{r}

model_04_rf_ranger$fit %>%
  ranger::importance() %>%
  enframe() %>%
  
  arrange(desc(value)) %>%
  mutate(name = as_factor(name) %>% fct_rev()) %>%
     
  ggplot(aes(value, name)) + 
    geom_point() + 
    ggrepel::geom_label_repel(aes(label = scales::percent(value, accuracy = 1)),
                              size = 3) +
    labs(title = "ranger: Variable Importance",
         subtitle = "Model 05: Ranger Random Forest Model") +
    scale_x_continuous(labels = scales::percent_format())

```

### Model XGBOOST ----

```{r}
library(xgboost)
?boost_tree
?xgboost::xgboost

set.seed(1234)

model_07_boost_xgboost <- boost_tree("regression") %>%
    set_engine("xgboost",
               mtry= 30,
               learn_rate = 0.25,
               tree_depth=7, objective = 'reg:squarederror') %>%
  fit(Sharewomen~., data = train_data %>% select(-Rank, -Major,-Major_code, -Men, -Part_time))

model_07_boost_xgboost %>% calc_metrics(test_data)

```

### 4.3.2 Feature Importance ----


```{r}

model_07_boost_xgboost$fit %>%
  xgboost::xgb.importance(model = .) %>%
  as_tibble() %>%
  
  arrange(desc(Gain)) %>%
  mutate(Feature = as_factor(Feature) %>% fct_rev()) %>%
  
  ggplot(aes(Gain, Feature)) +
  geom_point() +
  ggrepel::geom_label_repel(aes(label = scales::percent(Gain, accuracy = 1)),
                              size = 3) +
  
  labs(
        title = "Xgboost: Variable Importance",
        subtitle = "Model 07: XGboost Model"
    )

```

OUr XGBOOST metrics outperformed all the above models. This model explained 99% of the data and as we can see, the Mean Absolute Error & Root Mean Square Error are considerably smaller than the others. 



